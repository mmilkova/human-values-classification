{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_path = \"\"  # Navigate to the main folder where the data fragments are located and combine them into a single file.\n",
    "all_dfs = []\n",
    "\n",
    "for root, _, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            df = pd.read_csv(file_path, sep=\"|\")\n",
    "            all_dfs.append(df)\n",
    "\n",
    "# Combine to a single df\n",
    "concatenated_df = pd.concat(all_dfs, ignore_index=True)\n",
    "concatenated_df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language selection. We only accept Cyrillic characters.\n",
    "import fasttext\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "model = fasttext.load_model(\"lid.176.bin\")\n",
    "\n",
    "def detect_lang(text):\n",
    "    pred = model.predict(text.replace(\"\\n\", \" \"), k=1)\n",
    "    return pred[0][0].replace(\"__label__\", \"\")\n",
    "\n",
    "concatenated_df['lang'] = concatenated_df['text'].apply(lambda x: detect_lang(x) if isinstance(x, str) else '')\n",
    "df = concatenated_df[concatenated_df['lang'] == 'ru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6566535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a title and description of the link to the post text if it was reposted:\n",
    "\n",
    "df = df.fillna('')\n",
    "df['text']=df['text'].str.replace('Запись удалена', \"\")\n",
    "df['repost_text']=df['repost_text'].str.replace('Запись удалена', \"\")\n",
    "# df['history_text']=df['history_text'].str.replace('Запись удалена', \"\")\n",
    "\n",
    "def merge_texts(row):\n",
    "    texts = [row[\"text\"]]  # Always start with the text of the post\n",
    "\n",
    "    if row[\"link_title\"] and row[\"link_title\"] != row[\"text\"]:\n",
    "        texts.append(row[\"link_title\"])  # Add a link title if it does not match the post text\n",
    "    \n",
    "    if row[\"link_description\"] and row[\"link_description\"] not in texts:\n",
    "        texts.append(row[\"link_description\"])  # Add a description for the link if it is not already there.\n",
    "    \n",
    "    if row[\"repost_text\"] and row[\"repost_text\"] not in texts:\n",
    "        texts.append(row[\"repost_text\"])\n",
    "    \n",
    "#     if row[\"history_text\"] and row[\"history_text\"] not in texts:\n",
    "#         texts.append(row[\"history_text\"]) \n",
    "        \n",
    "    return \". \".join(map(str, texts)).strip()\n",
    "\n",
    "df[\"text\"] = df.apply(merge_texts, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00015c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['link_description', 'repost_text', 'history_text', 'lang'], axis=1, inplace=True)\n",
    "df.drop(['link_description', 'repost_text', 'lang'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Lemmatization\n",
    "import re\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")   \n",
    "rsw = [word for word in russian_stopwords if word !=\"не\"]\n",
    "\n",
    "tokenizer = RegexpTokenizer('[А-Я|Ё|а-я|ё|#]+')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def preprocessing(plain_text):\n",
    "    intermediate = tokenizer.tokenize(plain_text.lower())\n",
    "    intermediate = [morph.parse(i)[0].normal_form for i in intermediate if (len(i)>1)&('#' not in i)]\n",
    "    words_lemmatized_list = [i for i in intermediate if i not in rsw]\n",
    "    return words_lemmatized_list\n",
    "\n",
    "def form_bigrams_list(preprocessed_text):\n",
    "    bigrams_list=[]\n",
    "    biword =  [b for b in nltk.bigrams(preprocessed_text)]\n",
    "    counts_bi = Counter(biword)\n",
    "    for char in counts_bi.keys():\n",
    "        bigrams_list.append('_'.join(char))\n",
    "    return bigrams_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def process_text(text):\n",
    "    preprocessed_text = preprocessing(text)\n",
    "    biword = form_bigrams_list(preprocessed_text)\n",
    "    return preprocessed_text, biword\n",
    "\n",
    "df[['unigrams', 'bigrams']] = df['text'].progress_apply(lambda x: process_text(x)).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5fbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ !!! !! Add removal of ID and other personal information in post texts!!!   ============\n",
    "import re\n",
    "\n",
    "def clean_personal_data(text):\n",
    "    \"\"\"Removes from texts: IDs, mentions, links, phone numbers, email addresses, and card numbers.\"\"\"\n",
    "    \n",
    "    # Removing user and group IDs\n",
    "    text = re.sub(r'\\b(id|club)\\d+\\b', '', text)\n",
    "    \n",
    "    # Removing mentions via @\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    \n",
    "    # Removing links to VK pages\n",
    "    text = re.sub(r'https?://(?:m\\.)?vk\\.com/\\S+', '[LINK]', text)\n",
    "    \n",
    "    # Removing phone numbers\n",
    "    text = re.sub(r'(\\+7|8)[\\s\\-]?\\(?\\d{3}\\)?[\\s\\-]?\\d{2,3}[\\s\\-]?\\d{2,3}[\\s\\-]?\\d{2,4}', '[PHONE]', text)\n",
    "    \n",
    "    # Removing email addresses\n",
    "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '[EMAIL]', text)\n",
    "    \n",
    "    # removing card numbers\n",
    "    text = re.sub(r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b', '[CARD]', text)\n",
    "\n",
    "    return text.strip()  # Remove extra spaces\n",
    "\n",
    "df['text'] = df['text'].apply(clean_personal_data)\n",
    "\n",
    "df.to_csv(folder+file, sep='|', encoding='utf-8')  # add you fold and file name\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
