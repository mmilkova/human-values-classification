{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d29530ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-large\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_scheduler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "# ============================\n",
    "# 1. Load models:\n",
    "# ============================\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "MODEL_NAME = \"FacebookAI/xlm-roberta-large\"  #\"ai-forever/ruRoberta-large\" \"sergeyzh/BERTA\" #\"FacebookAI/xlm-roberta-large\"  \"ai-forever/ruBert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(MODEL_NAME).to(\"cuda:1\") \n",
    "model_name_to_save=MODEL_NAME.split('/')[1]\n",
    "print (model_name_to_save)\n",
    "\n",
    "\n",
    "def unfreeze_last_layer(model): # set which layers to freeze\n",
    "    for name, param in model.bert.named_parameters():\n",
    "        if any(layer in name for layer in ['encoder.layer.22','encoder.layer.23']):  #'pooler'#'encoder.layer.9', 'encoder.layer.10', \n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "            \n",
    "            \n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tfidf_features, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tfidf_features = tfidf_features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        tfidf_vector = torch.tensor(self.tfidf_features[idx], dtype=torch.float32)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text, padding=\"max_length\", truncation=True, \n",
    "            max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"tfidf_features\": tfidf_vector\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Define the model\n",
    "# ============================\n",
    "class RuBERTWithTFIDF(nn.Module):\n",
    "    def __init__(self, bert_model, tfidf_dim, num_labels=10):\n",
    "        super(RuBERTWithTFIDF, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.tfidf_layer = nn.Linear(tfidf_dim, 128)  #128\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm = nn.LayerNorm(self.bert.config.hidden_size + 128)  \n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + 128, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tfidf_features):\n",
    "         \n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        tfidf_embedding = self.relu(self.tfidf_layer(tfidf_features))\n",
    "        concat = torch.cat((bert_output, tfidf_embedding), dim=1)\n",
    "        concat = self.batch_norm(concat)  \n",
    "        logits = self.classifier(self.dropout(concat))\n",
    "        return logits  \n",
    "\n",
    "\n",
    "class BERTWithMeanPoolingTFIDF(nn.Module):\n",
    "    def __init__(self, model_name, tfidf_dim, num_labels=10):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.tfidf_layer = nn.Linear(tfidf_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(self.bert.config.hidden_size + 128)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + 128, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tfidf_features):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        mean_pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "        tfidf_proj = self.relu(self.tfidf_layer(tfidf_features))\n",
    "        combined = torch.cat((mean_pooled, tfidf_proj), dim=1)\n",
    "        normed = self.norm(combined)\n",
    "        logits = self.classifier(self.dropout(normed))\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4dc7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_idh</th>\n",
       "      <th>post_idh</th>\n",
       "      <th>Self-direction</th>\n",
       "      <th>Stimulation</th>\n",
       "      <th>Hedonism</th>\n",
       "      <th>Achievement</th>\n",
       "      <th>Power</th>\n",
       "      <th>Security</th>\n",
       "      <th>Conformity</th>\n",
       "      <th>Tradition</th>\n",
       "      <th>Benevolence</th>\n",
       "      <th>Universalism</th>\n",
       "      <th>If_political</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e9deaf0313932da383cea726f92f9eb3d1c628877d3424...</td>\n",
       "      <td>c044a467d4ac05d966e48ee9df3413ac3727b2125e90f0...</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c5fe892dc5e0594c0e752e9b09619507e46c19d8908452...</td>\n",
       "      <td>bc6b38f59d571d80de7a08b248917e049d2af88f2c5f33...</td>\n",
       "      <td>0,0,1,1,0</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>323c26a3aa3b8924ae899c8e25dd28437f198cf1db553f...</td>\n",
       "      <td>cb95507ecab36396c8c9a31abc9ec0e2b44f4d1cc0152a...</td>\n",
       "      <td>1,1,1,0,1</td>\n",
       "      <td>0,1,1,0,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,0,1,1</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,1,0,1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2e15352864c7921f447a7df8c25998aa52b2e70c882f37...</td>\n",
       "      <td>6b1d3f8627aac562aa08965624d1ccc333a45a53d0b405...</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,1,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,0,0,0,0</td>\n",
       "      <td>0,1,1,1,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66ab20bb3078aaeb5f904a37313509c77aef6a2c632013...</td>\n",
       "      <td>aedbab70bbbca94d30c00e7bf336d93c67390e9939dad5...</td>\n",
       "      <td>1,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>1,1,0,1,0</td>\n",
       "      <td>1,1,0,0,0</td>\n",
       "      <td>1,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,1,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>d9be01ec25121101f7125646dd78950f17c2b59380b597...</td>\n",
       "      <td>de5e4b3ccf1d5466c6abd047610f2f2d30b6cc930af881...</td>\n",
       "      <td>1,1,0,1,1</td>\n",
       "      <td>0,0,0,0,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,1,1,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,1,1,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>8c2ae840d8aeddad8f237d98efeb671d7b30ca8e11fa4e...</td>\n",
       "      <td>ef56f00ee8c299eb843f0c663f3f17a4032d466eb76e42...</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,1,1,0,0</td>\n",
       "      <td>1,1,0,1,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>727090bd2f2925c877401cf8cd9e8ef29360a0fe76c9a9...</td>\n",
       "      <td>9013b99714b49d6a19292627b108ba30a1d60b33088c90...</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>0,0,0,0,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>1,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,1,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,1,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>66ab20bb3078aaeb5f904a37313509c77aef6a2c632013...</td>\n",
       "      <td>a5646486dad330c70811bac7bf3c8b3a1ad619a01c570e...</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,0,0,0,0</td>\n",
       "      <td>1,1,0,0,1</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>0,0,1,0,0</td>\n",
       "      <td>1,0,1,0,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>b95bf6fa96adf978a030ffd8340edea9fded98ef49170e...</td>\n",
       "      <td>06f5a899f338d00e3086755d55d0cd443017b5c6c6d3ee...</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>0,0,0,1,0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                user_idh  \\\n",
       "0      e9deaf0313932da383cea726f92f9eb3d1c628877d3424...   \n",
       "1      c5fe892dc5e0594c0e752e9b09619507e46c19d8908452...   \n",
       "2      323c26a3aa3b8924ae899c8e25dd28437f198cf1db553f...   \n",
       "3      2e15352864c7921f447a7df8c25998aa52b2e70c882f37...   \n",
       "4      66ab20bb3078aaeb5f904a37313509c77aef6a2c632013...   \n",
       "...                                                  ...   \n",
       "19995  d9be01ec25121101f7125646dd78950f17c2b59380b597...   \n",
       "19996  8c2ae840d8aeddad8f237d98efeb671d7b30ca8e11fa4e...   \n",
       "19997  727090bd2f2925c877401cf8cd9e8ef29360a0fe76c9a9...   \n",
       "19998  66ab20bb3078aaeb5f904a37313509c77aef6a2c632013...   \n",
       "19999  b95bf6fa96adf978a030ffd8340edea9fded98ef49170e...   \n",
       "\n",
       "                                                post_idh Self-direction  \\\n",
       "0      c044a467d4ac05d966e48ee9df3413ac3727b2125e90f0...      1,1,1,1,1   \n",
       "1      bc6b38f59d571d80de7a08b248917e049d2af88f2c5f33...      0,0,1,1,0   \n",
       "2      cb95507ecab36396c8c9a31abc9ec0e2b44f4d1cc0152a...      1,1,1,0,1   \n",
       "3      6b1d3f8627aac562aa08965624d1ccc333a45a53d0b405...      0,0,0,0,0   \n",
       "4      aedbab70bbbca94d30c00e7bf336d93c67390e9939dad5...      1,0,0,0,0   \n",
       "...                                                  ...            ...   \n",
       "19995  de5e4b3ccf1d5466c6abd047610f2f2d30b6cc930af881...      1,1,0,1,1   \n",
       "19996  ef56f00ee8c299eb843f0c663f3f17a4032d466eb76e42...      0,0,0,0,0   \n",
       "19997  9013b99714b49d6a19292627b108ba30a1d60b33088c90...      1,1,1,1,1   \n",
       "19998  a5646486dad330c70811bac7bf3c8b3a1ad619a01c570e...      0,0,0,0,0   \n",
       "19999  06f5a899f338d00e3086755d55d0cd443017b5c6c6d3ee...      0,0,0,0,0   \n",
       "\n",
       "      Stimulation   Hedonism Achievement      Power   Security Conformity  \\\n",
       "0       1,1,1,1,1  1,1,1,1,1   0,0,0,0,0  0,0,0,0,0  0,0,0,0,0  0,0,0,0,0   \n",
       "1       1,1,1,1,1  1,1,1,1,1   0,0,0,0,0  0,0,0,0,0  0,0,0,0,0  0,0,0,0,0   \n",
       "2       0,1,1,0,1  0,0,0,0,0   1,1,0,1,1  1,1,1,1,1  0,0,0,0,0  0,0,0,0,0   \n",
       "3       0,0,0,0,0  0,0,0,0,1   0,0,0,0,0  0,0,0,0,0  0,0,1,0,0  0,0,0,0,0   \n",
       "4       0,0,0,0,0  0,0,0,0,0   1,1,1,1,1  1,1,0,1,0  1,1,0,0,0  1,0,0,0,0   \n",
       "...           ...        ...         ...        ...        ...        ...   \n",
       "19995   0,0,0,0,1  0,0,0,0,0   0,0,0,0,0  0,0,0,0,0  0,0,0,0,0  0,1,1,0,0   \n",
       "19996   0,0,0,0,0  0,0,0,0,0   0,0,0,0,0  0,1,1,0,0  1,1,0,1,1  0,0,0,0,0   \n",
       "19997   0,0,0,0,1  0,0,0,0,0   1,1,1,1,1  1,0,0,0,0  0,0,0,0,0  0,1,0,0,0   \n",
       "19998   0,0,0,0,0  0,0,0,0,0   1,0,0,0,0  1,1,0,0,1  1,1,1,1,1  0,0,1,0,0   \n",
       "19999   0,0,0,0,0  0,0,0,0,0   0,0,0,0,0  0,0,0,0,0  0,0,0,0,1  0,0,0,0,0   \n",
       "\n",
       "       Tradition Benevolence Universalism  If_political  \n",
       "0      0,0,0,0,0   1,1,1,1,1    0,0,0,0,0           0.0  \n",
       "1      0,0,0,0,0   1,1,0,0,0    0,0,0,0,0           0.0  \n",
       "2      0,0,0,0,0   0,0,0,0,0    0,0,1,0,1           1.0  \n",
       "3      1,0,0,0,0   0,1,1,1,1    0,0,0,0,0           0.0  \n",
       "4      0,0,0,0,0   0,0,1,0,0    0,0,0,0,0           1.0  \n",
       "...          ...         ...          ...           ...  \n",
       "19995  0,0,0,0,0   1,1,1,1,0    0,0,0,0,0           0.0  \n",
       "19996  0,0,0,0,0   0,0,0,0,0    0,0,0,0,1           1.0  \n",
       "19997  0,0,0,0,0   0,1,0,0,0    0,0,0,0,0           0.0  \n",
       "19998  1,0,1,0,1   0,0,0,0,0    1,1,1,1,1           1.0  \n",
       "19999  0,0,0,0,0   1,1,1,1,1    0,0,0,1,0           0.0  \n",
       "\n",
       "[20000 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "res_folder='../data/annotations/'\n",
    "df=pd.read_csv(res_folder+'gpt-labels-20k.csv', sep=\"|\", encoding ='utf-8')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b80eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_list=['Self-direction','Stimulation','Hedonism','Achievement','Power','Security','Conformity','Tradition','Benevolence','Universalism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ad6871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Post texts have been removed from public access\n",
    "texts = df.text.tolist()\n",
    "annotations_ = df[values_list].applymap(lambda x: list(map(float, str(x).split(\",\"))))\n",
    "annotations=annotations_.applymap(lambda x: x if np.isnan(x).any()==False else [0,0,0,0,0]).values # где-то есть nan - заменяем на 0\n",
    "\n",
    "# convert to soft labels\n",
    "\n",
    "def parse_and_aggregate(label_string):\n",
    "    labels = list(map(float, label_string.split(\",\")))  \n",
    "    sum_labels = sum(labels) \n",
    "    if sum_labels == 5:\n",
    "        return 1\n",
    "    elif sum_labels == 4:\n",
    "        return 1\n",
    "    elif sum_labels == 3:\n",
    "        return 0.6\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "categorized_annotations = df[values_list].applymap(parse_and_aggregate).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bc8a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split to train/valid\n",
    "X_train_texts, X_val_texts, train_labels, val_labels= train_test_split(df, categorized_annotations, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6e79d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# TF-IDF + BERT tokenization)\n",
    "# ============================\n",
    "vectorizer = TfidfVectorizer(max_features=5000, max_df=0.8, min_df=2, ngram_range=(1,2))\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_texts.text.to_list()).toarray()\n",
    "X_val_tfidf = vectorizer.transform(X_val_texts.text.to_list()).toarray()\n",
    "\n",
    "train_dataset = MultiLabelDataset(X_train_texts.text.to_list(), train_labels, X_train_tfidf, tokenizer)\n",
    "val_dataset = MultiLabelDataset(X_val_texts.text.to_list(), val_labels, X_val_tfidf, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f6d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# save TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, \"../models/xlm-roberta-large/tfidf_vectorizer_for_train_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b57e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95e77d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: tensor([0.2388, 0.4283, 0.6886, 0.5962, 0.6953, 0.7139, 1.0000, 0.7191, 0.2712,\n",
      "        0.6631], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "model = RuBERTWithTFIDF(bert_model, X_train_tfidf.shape[1]).to(device)\n",
    "unfreeze_last_layer(model)\n",
    "\n",
    "# ============================\n",
    "# Loss function and the optimizer\n",
    "# ============================\n",
    "\n",
    "# Class weights\n",
    "class_frequencies = np.sum(train_labels, axis=0) / train_labels.shape[0]\n",
    "class_weights = 1.0 / (class_frequencies + 1e-5)\n",
    "class_weights /= np.max(class_weights)  # normalize weights\n",
    "\n",
    "# convert weights to tensor for PyTorch\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Class Weights:\", class_weights_tensor)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, class_weights, gamma=0.3):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.class_weights = class_weights  \n",
    "        self.gamma = gamma\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")  \n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        bce_loss = self.bce(outputs, targets)\n",
    "        p_t = torch.exp(-bce_loss)  \n",
    "        \n",
    "        # Focal Loss\n",
    "        focal_loss = (1 - p_t) ** self.gamma * bce_loss\n",
    "\n",
    "        weighted_focal_loss = focal_loss * self.class_weights.to(outputs.device)\n",
    "\n",
    "        return weighted_focal_loss.mean()\n",
    "\n",
    "criterion = FocalLoss(class_weights=class_weights_tensor, gamma=0.3)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=10 * len(train_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b828cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Training model\n",
    "# ============================\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=5, device=device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=True)\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            tfidf_features = batch[\"tfidf_features\"].to(device)\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, tfidf_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {total_loss/len(train_loader)}\")\n",
    "        validate(model, val_loader, criterion, device)  #!!!\n",
    "        save_path = \"../models/\"+model_name_to_save+\"/model_finetuned_\"+model_name_to_save+\"_gamma0_3_\"+str(epoch+1)+\".pth\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Validation (F1-macro, F1-binary)\n",
    "# ============================\n",
    "def validate(model, val_loader, criterion, device=device):  \n",
    "    model.eval()\n",
    "    total_loss_val=0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:   \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            tfidf_features = batch[\"tfidf_features\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, tfidf_features) \n",
    "            \n",
    "            labels = batch[\"labels\"].to(device).float()\n",
    "\n",
    "            loss_val = criterion(outputs, labels)\n",
    "            total_loss_val += loss_val.item()\n",
    "            \n",
    "            all_preds.append(outputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    avg_loss_val = total_loss_val / len(val_loader)  #!!!\n",
    "    print(f\"Validation Loss: {avg_loss_val:.4f}\")\n",
    "    \n",
    "#     only for cpu:\n",
    "#     all_preds = np.vstack(all_preds)\n",
    "#     all_labels = np.vstack(all_labels)\n",
    "\n",
    "#    Only gor gpu:\n",
    "    all_preds = np.vstack([t.cpu().numpy() for t in all_preds])  \n",
    "    all_labels = np.vstack([t.cpu().numpy() for t in all_labels])\n",
    "    all_preds = torch.sigmoid(torch.tensor(all_preds)).numpy()\n",
    "\n",
    "    # convert to binary\n",
    "    binary_preds = (all_preds > 0.5).astype(int)\n",
    "    binary_labels = (all_labels > 0.5).astype(int)\n",
    "    \n",
    "    f1_macro = f1_score(binary_labels, binary_preds, average='macro')\n",
    "    f1_binary = f1_score(binary_labels, binary_preds, average='micro')\n",
    "    \n",
    "    print(f\"Validation F1-macro: {f1_macro:.4f}, F1-binary: {f1_binary:.4f}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a92d0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_loader(model, data_loader, device=device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            tfidf_features = batch[\"tfidf_features\"].to(device)\n",
    "            logits = model(input_ids, attention_mask, tfidf_features)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()         \n",
    "            all_preds.append(probs)\n",
    "    return np.vstack(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "290e0510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1000/1000 [11:56<00:00,  1.40it/s, loss=0.0836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.11739610173925757\n",
      "Validation Loss: 0.0964\n",
      "Validation F1-macro: 0.5262, F1-binary: 0.5819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1000/1000 [11:54<00:00,  1.40it/s, loss=0.0819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.09178518913686276\n",
      "Validation Loss: 0.0889\n",
      "Validation F1-macro: 0.6357, F1-binary: 0.6862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1000/1000 [11:52<00:00,  1.40it/s, loss=0.0592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.08014416098222137\n",
      "Validation Loss: 0.0873\n",
      "Validation F1-macro: 0.6428, F1-binary: 0.6819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1000/1000 [11:51<00:00,  1.40it/s, loss=0.0873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 0.06976181851699949\n",
      "Validation Loss: 0.0880\n",
      "Validation F1-macro: 0.6451, F1-binary: 0.6823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1000/1000 [11:51<00:00,  1.40it/s, loss=0.0431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 0.06022479407861829\n",
      "Validation Loss: 0.0951\n",
      "Validation F1-macro: 0.6637, F1-binary: 0.6931\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Launch training\n",
    "# ============================\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "069f778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! save model\n",
    "# import os\n",
    "# save_path = \"../models/\"+model_name_to_save+\"/\"+model_name_to_save+\"_finetuned.pth\"\n",
    "# torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4876fda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 250/250 [02:19<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Predict on validation data\n",
    "val_predictions = predict_from_loader(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c05d22",
   "metadata": {},
   "source": [
    "! Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b789445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "\n",
    "def find_best_thresholds(y_true, y_pred_probs, values_list):\n",
    "    best_thresholds = {}\n",
    "    y_true_binary=(val_labels >= 0.6).astype(int) \n",
    "    for i, value in enumerate(values_list):\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true_binary[:,i], y_pred_probs[:, i])\n",
    "        f1 = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-8)\n",
    "        idx = np.argmax(f1)\n",
    "        best_thresh = thresholds[idx] if idx < len(thresholds) else 0.5\n",
    "        best_thresholds[value] = round(float(best_thresh), 3)\n",
    "        \n",
    "    # Optimization of a single global threshold\n",
    "    flat_true = y_true_binary.flatten()\n",
    "    flat_probs = y_pred_probs.flatten()\n",
    "    thresholds = np.linspace(0.1, 0.9, 81)\n",
    "    best_thresh_global = thresholds[np.argmax([f1_score(flat_true, flat_probs > t) for t in thresholds])]\n",
    "    best_thresholds['GLOBAL'] = round(float(best_thresh_global), 3)\n",
    "    print(f\"Global threshold maximizing overall F1: {best_thresh_global:.3f}\")\n",
    "    \n",
    "    with open(\"../models/\"+model_name_to_save+\"/\"+model_name_to_save+\"_thresholds.json\", \"w\") as f:\n",
    "        json.dump({k: float(v) for k, v in best_thresholds.items()}, f, indent=2)\n",
    "    \n",
    "    return best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf2a382b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global threshold maximizing overall F1: 0.340\n"
     ]
    }
   ],
   "source": [
    "best_thresholds=find_best_thresholds(val_labels, val_predictions, values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4a4b137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Self-direction': 0.324,\n",
       " 'Stimulation': 0.241,\n",
       " 'Hedonism': 0.357,\n",
       " 'Achievement': 0.344,\n",
       " 'Power': 0.441,\n",
       " 'Security': 0.41,\n",
       " 'Conformity': 0.199,\n",
       " 'Tradition': 0.264,\n",
       " 'Benevolence': 0.465,\n",
       " 'Universalism': 0.425,\n",
       " 'GLOBAL': 0.34}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd7e0624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.907225\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94     33668\n",
      "           1       0.70      0.73      0.71      6332\n",
      "\n",
      "    accuracy                           0.91     40000\n",
      "   macro avg       0.82      0.84      0.83     40000\n",
      "weighted avg       0.91      0.91      0.91     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_predictions = (val_predictions >best_thresholds['GLOBAL']).astype(int)\n",
    "binary_y_val=(val_labels >= 0.6).astype(int)  #y_test\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(binary_y_val.flatten(), binary_predictions.flatten()))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(binary_y_val.flatten(), binary_predictions.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20295fe7",
   "metadata": {},
   "source": [
    "# Prediction for Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d111fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_idh</th>\n",
       "      <th>post_idh</th>\n",
       "      <th>Self-direction</th>\n",
       "      <th>Stimulation</th>\n",
       "      <th>Hedonism</th>\n",
       "      <th>Achievement</th>\n",
       "      <th>Power</th>\n",
       "      <th>Security</th>\n",
       "      <th>Conformity</th>\n",
       "      <th>Tradition</th>\n",
       "      <th>Benevolence</th>\n",
       "      <th>Universalism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4216821a041e700782114caac8cc624417097150ea89fd...</td>\n",
       "      <td>20f657e6b1850a8c930abc63417b7adb9d45002370a99c...</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,1,0,0,1</td>\n",
       "      <td>0,1,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deec49a0270cddec42ddb1279fb4130cd31ffe0bb335b8...</td>\n",
       "      <td>e2376a79db852eac32fb288b11e3fba92ac957047f7a53...</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,0,0,1,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,1,0,0</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>0,1,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70d9bf963dd27255db4bb4633ec8dc20cfef78476a2be8...</td>\n",
       "      <td>7bd7dc5d600a4845920e9912eabf18ddd0dcfdfccc60ba...</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,1,1,1,0</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7021e2ffb0f6020336fd9e6ec18c4cdb12b251fcd2c26...</td>\n",
       "      <td>db32d77983d3f4d52d9455a68c8ea958e73a88101859c6...</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04a0fc4429c6972b2c78b7550f925bcc4a9279d7238843...</td>\n",
       "      <td>3f26e05ad9a49f60b7e0cc328ee31d3a09385839e2e301...</td>\n",
       "      <td>0,0,1,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,1,1,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,1,1,1</td>\n",
       "      <td>1,1,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>15866c9fd3ed30399128d19cc5bc25a622d56aefd1a097...</td>\n",
       "      <td>51ceda055469d5a0e52b062eee744f5f9229a962af5c7e...</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>1,1,1,1,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,1,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>9e02685bb9281821fd892fb955e132c831b5d79d1212f6...</td>\n",
       "      <td>451475081033d2421ed5f2289237ea1f970ae5542cf355...</td>\n",
       "      <td>1,0,0,0,0</td>\n",
       "      <td>1,1,1,0,0</td>\n",
       "      <td>1,1,0,1,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,1,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>1ca9042d4634ddba81dc379bfb49dd4f769b77470c0022...</td>\n",
       "      <td>51ceda055469d5a0e52b062eee744f5f9229a962af5c7e...</td>\n",
       "      <td>1,0,1,0,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,1,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,1,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>a077c1420d5c151d18286711b9518a8923ab62afa7f488...</td>\n",
       "      <td>594e1ad708de94db2e876bca959251391aa0306dfbc19f...</td>\n",
       "      <td>1,0,1,1,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,1,1,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,1</td>\n",
       "      <td>1,1,1,1,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>05db116e438aea6fe216e51ca9d7dea1221a1579a3dccd...</td>\n",
       "      <td>8677a0233d1a8a4a3dd8cd443f445b625eff32ed8d0c24...</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,1,1,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "      <td>0,0,0,0,1</td>\n",
       "      <td>0,0,0,0,0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               user_idh  \\\n",
       "0     4216821a041e700782114caac8cc624417097150ea89fd...   \n",
       "1     deec49a0270cddec42ddb1279fb4130cd31ffe0bb335b8...   \n",
       "2     70d9bf963dd27255db4bb4633ec8dc20cfef78476a2be8...   \n",
       "3     e7021e2ffb0f6020336fd9e6ec18c4cdb12b251fcd2c26...   \n",
       "4     04a0fc4429c6972b2c78b7550f925bcc4a9279d7238843...   \n",
       "...                                                 ...   \n",
       "3995  15866c9fd3ed30399128d19cc5bc25a622d56aefd1a097...   \n",
       "3996  9e02685bb9281821fd892fb955e132c831b5d79d1212f6...   \n",
       "3997  1ca9042d4634ddba81dc379bfb49dd4f769b77470c0022...   \n",
       "3998  a077c1420d5c151d18286711b9518a8923ab62afa7f488...   \n",
       "3999  05db116e438aea6fe216e51ca9d7dea1221a1579a3dccd...   \n",
       "\n",
       "                                               post_idh Self-direction  \\\n",
       "0     20f657e6b1850a8c930abc63417b7adb9d45002370a99c...      0,0,0,0,0   \n",
       "1     e2376a79db852eac32fb288b11e3fba92ac957047f7a53...      0,0,0,0,0   \n",
       "2     7bd7dc5d600a4845920e9912eabf18ddd0dcfdfccc60ba...      0,0,0,0,0   \n",
       "3     db32d77983d3f4d52d9455a68c8ea958e73a88101859c6...      0,0,0,0,0   \n",
       "4     3f26e05ad9a49f60b7e0cc328ee31d3a09385839e2e301...      0,0,1,0,0   \n",
       "...                                                 ...            ...   \n",
       "3995  51ceda055469d5a0e52b062eee744f5f9229a962af5c7e...      0,0,0,0,0   \n",
       "3996  451475081033d2421ed5f2289237ea1f970ae5542cf355...      1,0,0,0,0   \n",
       "3997  51ceda055469d5a0e52b062eee744f5f9229a962af5c7e...      1,0,1,0,1   \n",
       "3998  594e1ad708de94db2e876bca959251391aa0306dfbc19f...      1,0,1,1,0   \n",
       "3999  8677a0233d1a8a4a3dd8cd443f445b625eff32ed8d0c24...      0,0,0,0,0   \n",
       "\n",
       "     Stimulation   Hedonism Achievement      Power   Security Conformity  \\\n",
       "0      0,0,0,0,0  0,0,0,0,0   0,0,0,0,0  0,0,0,0,0  0,1,0,0,1  0,1,0,0,0   \n",
       "1      0,0,0,0,0  0,0,0,0,0   0,0,0,0,0  0,0,0,0,0  1,0,0,1,1  0,0,0,0,0   \n",
       "2      0,0,0,0,0  0,0,0,0,0   0,0,0,0,0  0,0,0,0,0  0,0,0,0,0  0,0,0,0,0   \n",
       "3      0,0,0,0,0  1,1,0,0,0   0,0,0,0,0  0,0,0,0,0  0,0,0,0,0  0,0,0,0,0   \n",
       "4      0,0,0,0,0  0,0,0,0,0   0,0,1,1,0  0,0,0,0,0  0,0,0,0,0  0,0,0,0,0   \n",
       "...          ...        ...         ...        ...        ...        ...   \n",
       "3995   0,0,0,0,0  0,0,0,0,0   0,0,0,0,0  0,0,0,0,0  1,1,1,1,0  0,0,0,0,0   \n",
       "3996   1,1,1,0,0  1,1,0,1,0   0,0,0,0,0  0,0,0,0,0  0,0,0,0,0  0,0,0,0,0   \n",
       "3997   0,0,0,0,0  0,1,0,0,0   0,0,0,0,0  0,0,0,0,0  0,0,0,0,0  0,0,0,0,0   \n",
       "3998   0,0,0,0,0  0,1,1,0,0   0,0,0,0,0  0,0,0,0,0  0,0,0,0,0  0,0,0,0,0   \n",
       "3999   0,0,0,0,0  0,1,1,0,0   0,0,0,0,0  0,0,0,0,0  0,0,0,0,0  0,0,0,0,0   \n",
       "\n",
       "      Tradition Benevolence Universalism  \n",
       "0     0,0,0,0,0   1,1,1,1,1    0,0,0,0,0  \n",
       "1     0,0,1,0,0   1,1,1,1,1    0,1,0,0,0  \n",
       "2     0,1,1,1,0   1,1,1,1,1    0,0,0,0,0  \n",
       "3     0,0,0,0,0   1,1,1,1,1    0,0,0,0,0  \n",
       "4     0,0,0,0,0   1,1,1,1,1    1,1,0,0,0  \n",
       "...         ...         ...          ...  \n",
       "3995  0,0,0,0,0   0,0,0,1,0    0,0,0,0,0  \n",
       "3996  0,0,0,0,1   0,0,0,0,0    0,0,1,0,0  \n",
       "3997  0,0,0,0,0   0,0,0,0,0    0,0,0,1,0  \n",
       "3998  0,0,0,0,1   1,1,1,1,0    0,0,0,0,0  \n",
       "3999  0,0,0,0,0   0,0,0,0,1    0,0,0,0,0  \n",
       "\n",
       "[4000 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "res_folder='../data/annotations/'\n",
    "df_test=pd.read_csv(res_folder+'gpt-labels-testdata-4k.csv', sep=\"|\", encoding ='utf-8')\n",
    "df_test.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a83f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.6.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/mmilkov2/miniconda3/envs/TextMining_py3/lib/python3.8/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.6.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# load TF-IDF vectorizer\n",
    "vectorizer = joblib.load(\"../models/xlm-roberta-large/tfidf_vectorizer_for_train_data.pkl\")\n",
    "\n",
    "# Vectorize new data\n",
    "X_tfidf_new = vectorizer.transform(df_test[\"text\"]).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "12a45d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset_for_new_data(Dataset):\n",
    "    def __init__(self, texts, tfidf_features, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "#         self.labels = labels\n",
    "        self.tfidf_features = tfidf_features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        tfidf_vector = torch.tensor(self.tfidf_features[idx], dtype=torch.float32)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text, padding=\"max_length\", truncation=True, \n",
    "            max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"tfidf_features\": tfidf_vector\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ff01329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32903/1169341876.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  annotations_ = df_test[values_list].applymap(lambda x: list(map(float, str(x).split(\",\"))))\n",
      "/tmp/ipykernel_32903/1169341876.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  annotations=annotations_.applymap(lambda x: x if np.isnan(x).any()==False else [0,0,0,0,0]).values # где-то есть nan - заменяем на 0\n",
      "/tmp/ipykernel_32903/1169341876.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  categorized_annotations_test = df_test[values_list].applymap(parse_and_aggregate).to_numpy()\n"
     ]
    }
   ],
   "source": [
    "annotations_ = df_test[values_list].applymap(lambda x: list(map(float, str(x).split(\",\"))))\n",
    "annotations=annotations_.applymap(lambda x: x if np.isnan(x).any()==False else [0,0,0,0,0]).values \n",
    "\n",
    "categorized_annotations_test = df_test[values_list].applymap(parse_and_aggregate).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff66695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = MultiLabelDataset_for_new_data(df_test.text.to_list(), X_tfidf_new, tokenizer)\n",
    "new_loader = DataLoader(new_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4e4ecd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name=\"../models/\"+model_name_to_save+\"/model_finetuned_\"+model_name_to_save+\".pth\"\n",
    "model.load_state_dict(torch.load(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8fb833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 250/250 [02:19<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions_test_bert = predict_from_loader(model, new_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352104b",
   "metadata": {},
   "source": [
    "! Metrics for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f87c4104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Test Data: 0.910625\n",
      "Classification Report for Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95     33947\n",
      "           1       0.70      0.72      0.71      6053\n",
      "\n",
      "    accuracy                           0.91     40000\n",
      "   macro avg       0.82      0.83      0.83     40000\n",
      "weighted avg       0.91      0.91      0.91     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_predictions_test = (predictions_test_bert >best_thresholds['GLOBAL']).astype(int)\n",
    "binary_y_test=(categorized_annotations_test >= 0.6).astype(int)  #y_test\n",
    "\n",
    "print(\"Accuracy for Test Data:\", accuracy_score(binary_y_test.flatten(), binary_predictions_test.flatten()))\n",
    "print(\"Classification Report for Test Data:\")\n",
    "print(classification_report(binary_y_test.flatten(), binary_predictions_test.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7cadd99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining metrics for the validation and test data in a combined table\n",
    "\n",
    "def evaluate_model_dual(val_true, val_probs, test_true, test_probs, thresholds, values_list):\n",
    "    records = []\n",
    "    val_pred = (val_probs > np.array([thresholds[v] for v in values_list])).astype(int)\n",
    "    test_pred = (test_probs > np.array([thresholds[v] for v in values_list])).astype(int)\n",
    "\n",
    "    for i, name in enumerate(values_list):\n",
    "        val_true_binary=(val_true[:, i] >= 0.6).astype(int)\n",
    "        test_true_binary=(test_true[:, i] >= 0.6).astype(int)\n",
    "        f1_val = f1_score(val_true_binary, val_pred[:, i])\n",
    "        f1_test = f1_score(test_true_binary, test_pred[:, i])\n",
    "        f1_macro_val = f1_score(val_true_binary, val_pred[:, i], average='macro')\n",
    "        f1_macro_test = f1_score(test_true_binary, test_pred[:, i], average='macro')\n",
    "        acc_val = accuracy_score(val_true_binary, val_pred[:, i])\n",
    "        acc_test = accuracy_score(test_true_binary, test_pred[:, i])\n",
    "\n",
    "        records.append({\n",
    "            'Value': name,\n",
    "            'F1': f\"{f1_val:.3f} / {f1_test:.3f}\",\n",
    "            'F1-macro': f\"{f1_macro_val:.3f} / {f1_macro_test:.3f}\",\n",
    "            'Accuracy': f\"{acc_val:.3f} / {acc_test:.3f}\"\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(records)\n",
    "    df_results.to_csv( \"\", index=False)\n",
    "   \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "16ce0abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>F1</th>\n",
       "      <th>F1-macro</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Self-direction</td>\n",
       "      <td>0.758 / 0.768</td>\n",
       "      <td>0.814 / 0.823</td>\n",
       "      <td>0.831 / 0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stimulation</td>\n",
       "      <td>0.727 / 0.725</td>\n",
       "      <td>0.828 / 0.827</td>\n",
       "      <td>0.888 / 0.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hedonism</td>\n",
       "      <td>0.647 / 0.613</td>\n",
       "      <td>0.799 / 0.779</td>\n",
       "      <td>0.914 / 0.903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Achievement</td>\n",
       "      <td>0.725 / 0.693</td>\n",
       "      <td>0.843 / 0.825</td>\n",
       "      <td>0.932 / 0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Power</td>\n",
       "      <td>0.797 / 0.640</td>\n",
       "      <td>0.884 / 0.809</td>\n",
       "      <td>0.951 / 0.959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Security</td>\n",
       "      <td>0.689 / 0.628</td>\n",
       "      <td>0.822 / 0.797</td>\n",
       "      <td>0.922 / 0.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Conformity</td>\n",
       "      <td>0.512 / 0.518</td>\n",
       "      <td>0.735 / 0.740</td>\n",
       "      <td>0.923 / 0.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tradition</td>\n",
       "      <td>0.746 / 0.627</td>\n",
       "      <td>0.859 / 0.798</td>\n",
       "      <td>0.949 / 0.942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Benevolence</td>\n",
       "      <td>0.802 / 0.821</td>\n",
       "      <td>0.864 / 0.862</td>\n",
       "      <td>0.892 / 0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Universalism</td>\n",
       "      <td>0.615 / 0.604</td>\n",
       "      <td>0.781 / 0.782</td>\n",
       "      <td>0.908 / 0.927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Value             F1       F1-macro       Accuracy\n",
       "0  Self-direction  0.758 / 0.768  0.814 / 0.823  0.831 / 0.840\n",
       "1     Stimulation  0.727 / 0.725  0.828 / 0.827  0.888 / 0.886\n",
       "2        Hedonism  0.647 / 0.613  0.799 / 0.779  0.914 / 0.903\n",
       "3     Achievement  0.725 / 0.693  0.843 / 0.825  0.932 / 0.925\n",
       "4           Power  0.797 / 0.640  0.884 / 0.809  0.951 / 0.959\n",
       "5        Security  0.689 / 0.628  0.822 / 0.797  0.922 / 0.937\n",
       "6      Conformity  0.512 / 0.518  0.735 / 0.740  0.923 / 0.927\n",
       "7       Tradition  0.746 / 0.627  0.859 / 0.798  0.949 / 0.942\n",
       "8     Benevolence  0.802 / 0.821  0.864 / 0.862  0.892 / 0.875\n",
       "9    Universalism  0.615 / 0.604  0.781 / 0.782  0.908 / 0.927"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model_dual(val_labels, val_predictions, categorized_annotations_test, predictions_test_bert, best_thresholds, values_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
