{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_raw=# load your data\n",
    "# select only value-expressive and/or politically oriented posts:\n",
    "df=df_raw.loc[df_raw.if_political_OR_if_value==1]\n",
    "\n",
    "values_list=['Self-direction','Stimulation','Hedonism','Achievement','Power','Security','Conformity','Tradition','Benevolence','Universalism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12995c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# download TF-IDF vectorizer:\n",
    "vectorizer = joblib.load(\"../models/xlm-roberta-large/tfidf_vectorizer_for_train_data.pkl\")\n",
    "\n",
    "# vectorize new data:\n",
    "X_tfidf_new = vectorizer.transform(df[\"text\"]).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTWithTFIDF(nn.Module):\n",
    "    def __init__(self, bert_model, tfidf_dim, num_labels=10):\n",
    "        super(BERTWithTFIDF, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.tfidf_layer = nn.Linear(tfidf_dim, 128)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm = nn.LayerNorm(self.bert.config.hidden_size + 128)  \n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + 128, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tfidf_features):\n",
    "         \n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        tfidf_embedding = self.relu(self.tfidf_layer(tfidf_features))\n",
    "        concat = torch.cat((bert_output, tfidf_embedding), dim=1)\n",
    "        concat = self.batch_norm(concat)  \n",
    "        logits = self.classifier(self.dropout(concat))\n",
    "        return logits  \n",
    "    \n",
    "\n",
    "# Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, class_weights, gamma=0.3):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.class_weights = class_weights  # add class weights\n",
    "        self.gamma = gamma\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")  \n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        bce_loss = self.bce(outputs, targets)\n",
    "        p_t = torch.exp(-bce_loss)  \n",
    "        focal_loss = (1 - p_t) ** self.gamma * bce_loss\n",
    "        weighted_focal_loss = focal_loss * self.class_weights.to(outputs.device)\n",
    "        return weighted_focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b33138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ============================\n",
    "# 1. Load xlm-roberta-large\n",
    "# ============================\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "MODEL_NAME = \"FacebookAI/xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(MODEL_NAME).to(\"cuda:1\")  \n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ============================\n",
    "\n",
    "model_bert = BERTWithTFIDF(bert_model, 5000).to(device)\n",
    "\n",
    "# Load vodel weights:\n",
    "loaded_model=\"../models/xlm-roberta-large/model_finetuned_xlm-roberta-large.pth\" \n",
    "model_bert.load_state_dict(torch.load(loaded_model, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset_for_new_data(Dataset):\n",
    "    def __init__(self, texts, tfidf_features, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tfidf_features = tfidf_features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tfidf_vector = torch.tensor(self.tfidf_features[idx], dtype=torch.float32)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text, padding=\"max_length\", truncation=True, \n",
    "            max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "#             \"labels\": labels,\n",
    "            \"tfidf_features\": tfidf_vector\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d039643",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = MultiLabelDataset_for_new_data(df.text.to_list(), X_tfidf_new, tokenizer)\n",
    "new_loader = DataLoader(new_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7531f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def predict_from_loader(model, data_loader, device=device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            tfidf_features = batch[\"tfidf_features\"].to(device)\n",
    "            logits = model(input_ids, attention_mask, tfidf_features)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()  \n",
    "            all_preds.append(probs)\n",
    "\n",
    "    return np.vstack(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6486b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_from_loader(model_bert, new_loader)\n",
    "df[values_list]=predictions\n",
    "for value in values_list:\n",
    "    df[value] = df[value].round(3)\n",
    "\n",
    "# save df.to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
