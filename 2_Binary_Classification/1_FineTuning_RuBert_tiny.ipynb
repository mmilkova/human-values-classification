{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3522406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup # AdamW (deprec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30103e31",
   "metadata": {},
   "source": [
    "# 1. Fine-tune rubert-tiny2 for binary classification (if the post express any value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4b1fb",
   "metadata": {},
   "source": [
    "batch_size = 8, epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf9506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model\n",
    "BASE_BERT = 'cointegrated/rubert-tiny2'\n",
    "\n",
    "# data balance\n",
    "major_class_exrta_w = 1.2\n",
    "\n",
    "# data loading\n",
    "max_length = int(2048*0.75)\n",
    "batch_size = 8\n",
    "\n",
    "# train\n",
    "epochs = 5\n",
    "\n",
    "#not random\n",
    "seed_val = 42\n",
    "\n",
    "#save\n",
    "bert_path = f'./temp/models/v2_{batch_size}_{max_length}_{major_class_exrta_w}/'\n",
    "\n",
    "###################\n",
    "# compute on GPU #0\n",
    "device_id = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c8b8ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(bert_path)\n",
    "    os.mkdir(bert_path+'epochs')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(bert_path+'epochs')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1dea7e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:{}\".format(str(device_id)) if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497c159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5035\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Думаете, что умеете пользоваться фотошопом?...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...Самое страшное - это когда ты стоишь под х...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Друзья мои! Поддержим дочку моей подруги! Про...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Мой новый дневник, читаем, коментим :)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>РУССКИЙ  КРЫМ - МИФ для быдла! (о чем молчат ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0     Думаете, что умеете пользоваться фотошопом?...    0.0\n",
       "1   ...Самое страшное - это когда ты стоишь под х...    1.0\n",
       "2   Друзья мои! Поддержим дочку моей подруги! Про...    1.0\n",
       "3             Мой новый дневник, читаем, коментим :)    0.0\n",
       "4   РУССКИЙ  КРЫМ - МИФ для быдла! (о чем молчат ...    0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is no public data\n",
    "\n",
    "df=pd.read_csv('', sep=\"|\", encoding ='utf-8')[['text', 'final_label']] \n",
    "df=df.rename(columns={'final_label':'label'})\n",
    "print (df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7e4d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label = df.label.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d71fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3301\n",
       "1    1734\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine 'doesn't reflect' and 'spam' classes\n",
    "df.label=df.label.replace(3, 0)\n",
    "df.label.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69c2d9",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5543afb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df.label.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "792bd120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.label.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4209a8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63553974, 1.45184544])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_w = compute_class_weight('balanced', classes=df.label.unique(), y=df.label)\n",
    "\n",
    "# fix disbalance to major class\n",
    "major_class_idx = np.argmin(class_w)\n",
    "class_w[major_class_idx] = class_w[major_class_idx]/major_class_exrta_w\n",
    "class_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dcb2b3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5035,), (5035,))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.text\n",
    "Y = df.label\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c2a05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X.values, Y.values, test_size = .20, stratify = Y.values, random_state = 87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0be8557f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4028,), (1007,), 5035)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape , X_val.shape , X_train.shape[0] + X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9277df1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.74 s, sys: 128 ms, total: 4.87 s\n",
      "Wall time: 5.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BASE_BERT, \n",
    "                                          do_lower_case=False)\n",
    "                                          \n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    X_train, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length', # можно поставить True  #'max_length'\n",
    "    truncation=True,\n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    X_val, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',  #'max_length',\n",
    "    truncation=True,\n",
    "    max_length=max_length, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae5d602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(y_train)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(y_val)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4e44fb8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2048, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(BASE_BERT,\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cd1198d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), #SequentialSampler\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c29014cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "33794116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted'), f1_score(labels_flat, preds_flat, average='macro')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)} = {len(y_preds[y_preds==label])/len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca838004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5d748c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "#         batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0].to(device),\n",
    "                  'attention_mask': batch[1].to(device),\n",
    "                  'labels':         batch[2].to(device),\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c031e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "def save_checpoint(model, optimizer, output_model):\n",
    "    # save\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()}, output_model)\n",
    "\n",
    "# save(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "67411466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2148685f5c340398970074df279d6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "Training loss: 0.6455677735900122\n",
      "Validation loss: 0.6234295635469376\n",
      "F1 Score (Weighted/Macro): (0.656251957711443, 0.6468783185467086)\n",
      "Class: 0\n",
      "Accuracy: 370/660 = 0.5606060606060606\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 284/347 = 0.8184438040345822\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.5627760663037262\n",
      "Validation loss: 0.5686113704291601\n",
      "F1 Score (Weighted/Macro): (0.7084040459663059, 0.6914517516401745)\n",
      "Class: 0\n",
      "Accuracy: 442/660 = 0.6696969696969697\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 264/347 = 0.760806916426513\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.5233887325794924\n",
      "Validation loss: 0.556405167849291\n",
      "F1 Score (Weighted/Macro): (0.7316920650851454, 0.7120496307531641)\n",
      "Class: 0\n",
      "Accuracy: 476/660 = 0.7212121212121212\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 255/347 = 0.7348703170028819\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.5005263998690579\n",
      "Validation loss: 0.563113215660292\n",
      "F1 Score (Weighted/Macro): (0.7297478046872117, 0.7099630338745637)\n",
      "Class: 0\n",
      "Accuracy: 475/660 = 0.7196969696969697\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 254/347 = 0.7319884726224783\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.4895340929340039\n",
      "Validation loss: 0.5619315552333045\n",
      "F1 Score (Weighted/Macro): (0.7324875421414411, 0.7124270358566647)\n",
      "Class: 0\n",
      "Accuracy: 479/660 = 0.7257575757575757\n",
      "\n",
      "Class: 1\n",
      "Accuracy: 253/347 = 0.729106628242075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(class_w), reduction = 'mean').to(device)\n",
    "scores = {}\n",
    "\n",
    "for epoch in tqdm(range(0,5)): # 1,epochs+1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "#         batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0].to(device),\n",
    "                  'attention_mask': batch[1].to(device),\n",
    "                  'labels':         batch[2].to(device),\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs['logits']\n",
    "        \n",
    "        \n",
    "        loss = criterion(logits, inputs['labels'])\n",
    "        loss_train_total += loss.item()    \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "                 \n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1_w, val_f1_macro  = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted/Macro): {(val_f1_w, val_f1_macro)}')\n",
    "    accuracy_per_class(predictions, true_vals)\n",
    "    \n",
    "    scores[epoch] = {'Training loss':loss_train_avg,\n",
    "                     'Validation loss':val_loss,\n",
    "                     'F1 Score (Weighted/Macro)':(val_f1_w, val_f1_macro) \n",
    "                    }\n",
    "    \n",
    "    # save checkpoint\n",
    "    save_checpoint(model, optimizer, bert_path+f'epochs/{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bert_path+f'epochs/scores.json', 'w') as f:\n",
    "    json.dump(scores, f)\n",
    "\n",
    "# load best\n",
    "\n",
    "best_epoch = np.argmin([scores[e]['Validation loss'] for e in scores]) + 1\n",
    "print(f'best_epoch #{best_epoch}')\n",
    "\n",
    "checkpoint = torch.load(bert_path+f'epochs/{best_epoch}', map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.save_pretrained(bert_path)\n",
    "tokenizer.save_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a1828b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./temp/models/v2_8_1536_1.2/'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! SAVE MODEL !\n",
    "model.save_pretrained(bert_path)\n",
    "tokenizer.save_pretrained(bert_path)\n",
    "bert_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c798683",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_path = '../models/fine-tuned_rubert-tiny2/v2_8_1536_1.2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b81e8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34a2bc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../TopicModel/Distilbert_classification/models/v2_8_1536_1.2/ were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(rc_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(rc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff48f21",
   "metadata": {},
   "source": [
    "Take embeddings from fine-tuned rubert-tiny2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6ac69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36cc8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bae82ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e4c4021e8f4c01b3666b2b7c59a678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5035 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55min 26s, sys: 1min 32s, total: 56min 58s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "emb_list = list()\n",
    "it = 0\n",
    "for s in tqdm(df.text.values):\n",
    "    emb_list.append(embed_bert_cls(s, model_emb, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9ed784a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.018464</td>\n",
       "      <td>-0.025443</td>\n",
       "      <td>-0.005412</td>\n",
       "      <td>-0.067210</td>\n",
       "      <td>-0.008098</td>\n",
       "      <td>-0.042848</td>\n",
       "      <td>0.048566</td>\n",
       "      <td>0.038053</td>\n",
       "      <td>-0.051750</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>-0.137592</td>\n",
       "      <td>-0.063010</td>\n",
       "      <td>-0.102285</td>\n",
       "      <td>-0.017897</td>\n",
       "      <td>0.058684</td>\n",
       "      <td>-0.000319</td>\n",
       "      <td>0.078772</td>\n",
       "      <td>-0.005704</td>\n",
       "      <td>0.041218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.087185</td>\n",
       "      <td>-0.013556</td>\n",
       "      <td>0.034466</td>\n",
       "      <td>-0.041398</td>\n",
       "      <td>-0.009335</td>\n",
       "      <td>0.038518</td>\n",
       "      <td>0.072062</td>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.072556</td>\n",
       "      <td>-0.010305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009245</td>\n",
       "      <td>0.031439</td>\n",
       "      <td>0.044652</td>\n",
       "      <td>0.031633</td>\n",
       "      <td>-0.081807</td>\n",
       "      <td>-0.021026</td>\n",
       "      <td>-0.011231</td>\n",
       "      <td>0.007678</td>\n",
       "      <td>-0.057428</td>\n",
       "      <td>0.005134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.121493</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>-0.038873</td>\n",
       "      <td>-0.034820</td>\n",
       "      <td>-0.033760</td>\n",
       "      <td>-0.023758</td>\n",
       "      <td>0.055283</td>\n",
       "      <td>-0.082305</td>\n",
       "      <td>-0.030336</td>\n",
       "      <td>-0.063620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074011</td>\n",
       "      <td>0.065398</td>\n",
       "      <td>0.025788</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.049230</td>\n",
       "      <td>-0.076790</td>\n",
       "      <td>0.114930</td>\n",
       "      <td>0.083010</td>\n",
       "      <td>0.026723</td>\n",
       "      <td>-0.053610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.020010</td>\n",
       "      <td>0.041242</td>\n",
       "      <td>-0.016424</td>\n",
       "      <td>-0.036992</td>\n",
       "      <td>-0.039034</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.005490</td>\n",
       "      <td>0.042822</td>\n",
       "      <td>-0.038598</td>\n",
       "      <td>-0.078856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016536</td>\n",
       "      <td>-0.062215</td>\n",
       "      <td>-0.011497</td>\n",
       "      <td>-0.130887</td>\n",
       "      <td>0.024420</td>\n",
       "      <td>0.049948</td>\n",
       "      <td>-0.068521</td>\n",
       "      <td>-0.050156</td>\n",
       "      <td>0.030229</td>\n",
       "      <td>0.039134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.026711</td>\n",
       "      <td>-0.061871</td>\n",
       "      <td>0.018863</td>\n",
       "      <td>-0.026486</td>\n",
       "      <td>0.046739</td>\n",
       "      <td>-0.060128</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>-0.053569</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>-0.096092</td>\n",
       "      <td>-0.059798</td>\n",
       "      <td>-0.059040</td>\n",
       "      <td>0.025562</td>\n",
       "      <td>0.083468</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.097353</td>\n",
       "      <td>0.100729</td>\n",
       "      <td>-0.048607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5030</th>\n",
       "      <td>0.090720</td>\n",
       "      <td>0.068379</td>\n",
       "      <td>0.007549</td>\n",
       "      <td>-0.037703</td>\n",
       "      <td>0.008094</td>\n",
       "      <td>-0.022078</td>\n",
       "      <td>0.057416</td>\n",
       "      <td>0.074114</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.036362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039732</td>\n",
       "      <td>-0.056965</td>\n",
       "      <td>-0.118393</td>\n",
       "      <td>0.032156</td>\n",
       "      <td>0.018497</td>\n",
       "      <td>0.021067</td>\n",
       "      <td>0.037554</td>\n",
       "      <td>0.101765</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.004840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>0.143055</td>\n",
       "      <td>0.031883</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-0.107946</td>\n",
       "      <td>-0.004803</td>\n",
       "      <td>-0.010487</td>\n",
       "      <td>0.043309</td>\n",
       "      <td>0.042991</td>\n",
       "      <td>-0.025998</td>\n",
       "      <td>0.029961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>0.031169</td>\n",
       "      <td>0.063458</td>\n",
       "      <td>-0.007555</td>\n",
       "      <td>-0.065752</td>\n",
       "      <td>0.068371</td>\n",
       "      <td>-0.064800</td>\n",
       "      <td>0.034165</td>\n",
       "      <td>0.053517</td>\n",
       "      <td>-0.010068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5032</th>\n",
       "      <td>0.124135</td>\n",
       "      <td>0.004982</td>\n",
       "      <td>-0.001954</td>\n",
       "      <td>-0.043581</td>\n",
       "      <td>-0.027300</td>\n",
       "      <td>-0.040936</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>-0.125258</td>\n",
       "      <td>-0.009013</td>\n",
       "      <td>-0.044418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051877</td>\n",
       "      <td>0.048784</td>\n",
       "      <td>0.106491</td>\n",
       "      <td>0.048471</td>\n",
       "      <td>-0.019057</td>\n",
       "      <td>-0.011693</td>\n",
       "      <td>0.046124</td>\n",
       "      <td>-0.033521</td>\n",
       "      <td>0.045135</td>\n",
       "      <td>-0.047027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5033</th>\n",
       "      <td>0.050855</td>\n",
       "      <td>0.037297</td>\n",
       "      <td>0.013623</td>\n",
       "      <td>-0.040512</td>\n",
       "      <td>-0.012929</td>\n",
       "      <td>-0.042989</td>\n",
       "      <td>0.068927</td>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>0.060874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009632</td>\n",
       "      <td>-0.044231</td>\n",
       "      <td>0.059576</td>\n",
       "      <td>-0.017738</td>\n",
       "      <td>-0.072191</td>\n",
       "      <td>-0.035146</td>\n",
       "      <td>-0.112178</td>\n",
       "      <td>0.042907</td>\n",
       "      <td>-0.006640</td>\n",
       "      <td>-0.064849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5034</th>\n",
       "      <td>0.008289</td>\n",
       "      <td>0.058433</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>-0.033897</td>\n",
       "      <td>-0.067209</td>\n",
       "      <td>-0.010444</td>\n",
       "      <td>0.017773</td>\n",
       "      <td>-0.102263</td>\n",
       "      <td>0.044758</td>\n",
       "      <td>0.110103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046597</td>\n",
       "      <td>0.076197</td>\n",
       "      <td>0.102915</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>-0.096565</td>\n",
       "      <td>-0.038375</td>\n",
       "      <td>-0.022783</td>\n",
       "      <td>-0.060977</td>\n",
       "      <td>-0.062677</td>\n",
       "      <td>-0.003916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5035 rows × 312 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.018464 -0.025443 -0.005412 -0.067210 -0.008098 -0.042848  0.048566   \n",
       "1     0.087185 -0.013556  0.034466 -0.041398 -0.009335  0.038518  0.072062   \n",
       "2     0.121493  0.024607 -0.038873 -0.034820 -0.033760 -0.023758  0.055283   \n",
       "3    -0.020010  0.041242 -0.016424 -0.036992 -0.039034  0.003855  0.005490   \n",
       "4     0.026711 -0.061871  0.018863 -0.026486  0.046739 -0.060128  0.027036   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5030  0.090720  0.068379  0.007549 -0.037703  0.008094 -0.022078  0.057416   \n",
       "5031  0.143055  0.031883  0.000566 -0.107946 -0.004803 -0.010487  0.043309   \n",
       "5032  0.124135  0.004982 -0.001954 -0.043581 -0.027300 -0.040936  0.006083   \n",
       "5033  0.050855  0.037297  0.013623 -0.040512 -0.012929 -0.042989  0.068927   \n",
       "5034  0.008289  0.058433  0.002247 -0.033897 -0.067209 -0.010444  0.017773   \n",
       "\n",
       "           7         8         9    ...       302       303       304  \\\n",
       "0     0.038053 -0.051750  0.016248  ...  0.013072 -0.137592 -0.063010   \n",
       "1    -0.126953  0.072556 -0.010305  ... -0.009245  0.031439  0.044652   \n",
       "2    -0.082305 -0.030336 -0.063620  ...  0.074011  0.065398  0.025788   \n",
       "3     0.042822 -0.038598 -0.078856  ... -0.016536 -0.062215 -0.011497   \n",
       "4     0.004546 -0.053569  0.005665  ... -0.000268 -0.096092 -0.059798   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5030  0.074114  0.012745  0.036362  ...  0.039732 -0.056965 -0.118393   \n",
       "5031  0.042991 -0.025998  0.029961  ...  0.030708  0.031169  0.063458   \n",
       "5032 -0.125258 -0.009013 -0.044418  ...  0.051877  0.048784  0.106491   \n",
       "5033  0.009016  0.003902  0.060874  ...  0.009632 -0.044231  0.059576   \n",
       "5034 -0.102263  0.044758  0.110103  ... -0.046597  0.076197  0.102915   \n",
       "\n",
       "           305       306       307       308       309       310       311  \n",
       "0    -0.102285 -0.017897  0.058684 -0.000319  0.078772 -0.005704  0.041218  \n",
       "1     0.031633 -0.081807 -0.021026 -0.011231  0.007678 -0.057428  0.005134  \n",
       "2     0.005126  0.049230 -0.076790  0.114930  0.083010  0.026723 -0.053610  \n",
       "3    -0.130887  0.024420  0.049948 -0.068521 -0.050156  0.030229  0.039134  \n",
       "4    -0.059040  0.025562  0.083468  0.007707  0.097353  0.100729 -0.048607  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5030  0.032156  0.018497  0.021067  0.037554  0.101765  0.023785  0.004840  \n",
       "5031 -0.007555 -0.065752  0.068371 -0.064800  0.034165  0.053517 -0.010068  \n",
       "5032  0.048471 -0.019057 -0.011693  0.046124 -0.033521  0.045135 -0.047027  \n",
       "5033 -0.017738 -0.072191 -0.035146 -0.112178  0.042907 -0.006640 -0.064849  \n",
       "5034  0.000366 -0.096565 -0.038375 -0.022783 -0.060977 -0.062677 -0.003916  \n",
       "\n",
       "[5035 rows x 312 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings for train dataset:\n",
    "labels = df[\"label\"]\n",
    "\n",
    "emb_list_ = [np.asarray(s) for s in emb_list]\n",
    "df_emb_=pd.DataFrame(emb_list_)\n",
    "df_emb_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd64815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- интересный новый сервис, где можно оставить...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>чет как-то нерадостно все это...особо на фоне...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Repost with . ・・・ жаль что быстро убежала!!!#...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#hellomyearth #дорогажизни #разорванноекольцо</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#ВтандемеСМамой#кактампробка#😁</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_test\n",
       "0   - интересный новый сервис, где можно оставить...           1\n",
       "1   чет как-то нерадостно все это...особо на фоне...           0\n",
       "2  #Repost with . ・・・ жаль что быстро убежала!!!#...           0\n",
       "3      #hellomyearth #дорогажизни #разорванноекольцо           0\n",
       "4                     #ВтандемеСМамой#кактампробка#😁           0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test dataset\n",
    "\n",
    "test_data=pd.read_csv('', sep=\"|\", encoding ='utf-8')[['text', 'final_label']]\n",
    "test_data=test_data.rename(columns={'final_label':'label_test'})\n",
    "print (test_data.shape[0])\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ace1a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    532\n",
       "1    272\n",
       "Name: label_test, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.replace({'label_test': {3: 0}}, inplace=True)\n",
    "test_data.label_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4b75170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510f905497134fa0aa4fd06621749e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/804 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 20s, sys: 17.4 s, total: 8min 37s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "emb_list_test = list()\n",
    "it = 0\n",
    "for s in tqdm(test_data.text.values):\n",
    "    emb_list_test.append(embed_bert_cls(s, model_emb, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b1d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.047982</td>\n",
       "      <td>-0.007989</td>\n",
       "      <td>0.019273</td>\n",
       "      <td>-0.064445</td>\n",
       "      <td>-0.047470</td>\n",
       "      <td>0.079242</td>\n",
       "      <td>-0.094084</td>\n",
       "      <td>0.037180</td>\n",
       "      <td>0.049691</td>\n",
       "      <td>-0.039148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050658</td>\n",
       "      <td>-0.002591</td>\n",
       "      <td>-0.030989</td>\n",
       "      <td>0.011156</td>\n",
       "      <td>-0.025440</td>\n",
       "      <td>0.075424</td>\n",
       "      <td>0.062284</td>\n",
       "      <td>0.021909</td>\n",
       "      <td>0.038568</td>\n",
       "      <td>-0.067602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.048708</td>\n",
       "      <td>0.038768</td>\n",
       "      <td>0.008993</td>\n",
       "      <td>-0.043480</td>\n",
       "      <td>-0.011525</td>\n",
       "      <td>-0.074801</td>\n",
       "      <td>0.068448</td>\n",
       "      <td>-0.029761</td>\n",
       "      <td>0.038171</td>\n",
       "      <td>-0.086254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015159</td>\n",
       "      <td>-0.086703</td>\n",
       "      <td>0.058923</td>\n",
       "      <td>-0.156870</td>\n",
       "      <td>0.034332</td>\n",
       "      <td>0.029744</td>\n",
       "      <td>-0.051479</td>\n",
       "      <td>0.020975</td>\n",
       "      <td>-0.005021</td>\n",
       "      <td>-0.045287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046934</td>\n",
       "      <td>-0.007776</td>\n",
       "      <td>-0.034438</td>\n",
       "      <td>-0.060966</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>-0.036679</td>\n",
       "      <td>0.076556</td>\n",
       "      <td>-0.046489</td>\n",
       "      <td>-0.026749</td>\n",
       "      <td>-0.019772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>-0.030804</td>\n",
       "      <td>-0.031761</td>\n",
       "      <td>-0.056468</td>\n",
       "      <td>0.068399</td>\n",
       "      <td>0.162177</td>\n",
       "      <td>-0.056490</td>\n",
       "      <td>0.092742</td>\n",
       "      <td>0.047726</td>\n",
       "      <td>-0.021333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.044973</td>\n",
       "      <td>-0.031754</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>-0.088200</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>0.086622</td>\n",
       "      <td>0.048924</td>\n",
       "      <td>-0.041500</td>\n",
       "      <td>-0.050908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058033</td>\n",
       "      <td>-0.037874</td>\n",
       "      <td>-0.066198</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>0.083422</td>\n",
       "      <td>0.040231</td>\n",
       "      <td>-0.079380</td>\n",
       "      <td>0.028450</td>\n",
       "      <td>0.032081</td>\n",
       "      <td>0.027410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019812</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.010675</td>\n",
       "      <td>-0.081504</td>\n",
       "      <td>-0.021689</td>\n",
       "      <td>-0.016962</td>\n",
       "      <td>0.031124</td>\n",
       "      <td>0.072162</td>\n",
       "      <td>-0.058702</td>\n",
       "      <td>-0.041161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032649</td>\n",
       "      <td>-0.088169</td>\n",
       "      <td>-0.050347</td>\n",
       "      <td>-0.059466</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>0.087201</td>\n",
       "      <td>-0.081285</td>\n",
       "      <td>0.027309</td>\n",
       "      <td>0.088323</td>\n",
       "      <td>-0.029691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>0.101509</td>\n",
       "      <td>-0.086583</td>\n",
       "      <td>0.037290</td>\n",
       "      <td>-0.097883</td>\n",
       "      <td>-0.067217</td>\n",
       "      <td>-0.012579</td>\n",
       "      <td>0.081658</td>\n",
       "      <td>0.014439</td>\n",
       "      <td>-0.060402</td>\n",
       "      <td>-0.037441</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020561</td>\n",
       "      <td>-0.054927</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>-0.058199</td>\n",
       "      <td>0.079025</td>\n",
       "      <td>0.088163</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.089325</td>\n",
       "      <td>0.038812</td>\n",
       "      <td>0.045627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.015678</td>\n",
       "      <td>-0.010740</td>\n",
       "      <td>-0.001362</td>\n",
       "      <td>-0.025951</td>\n",
       "      <td>-0.016946</td>\n",
       "      <td>0.006844</td>\n",
       "      <td>-0.015210</td>\n",
       "      <td>0.062379</td>\n",
       "      <td>0.035687</td>\n",
       "      <td>-0.057801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027969</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>-0.050198</td>\n",
       "      <td>-0.040739</td>\n",
       "      <td>0.081978</td>\n",
       "      <td>0.084395</td>\n",
       "      <td>0.013094</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.059930</td>\n",
       "      <td>-0.015127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>0.017429</td>\n",
       "      <td>-0.018824</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>-0.043123</td>\n",
       "      <td>0.043978</td>\n",
       "      <td>-0.012925</td>\n",
       "      <td>-0.038624</td>\n",
       "      <td>0.043852</td>\n",
       "      <td>-0.117673</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>-0.078440</td>\n",
       "      <td>-0.072294</td>\n",
       "      <td>-0.057007</td>\n",
       "      <td>0.068025</td>\n",
       "      <td>0.063575</td>\n",
       "      <td>-0.018018</td>\n",
       "      <td>0.005897</td>\n",
       "      <td>0.067275</td>\n",
       "      <td>-0.090942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>-0.004265</td>\n",
       "      <td>0.044574</td>\n",
       "      <td>0.046853</td>\n",
       "      <td>-0.093753</td>\n",
       "      <td>-0.026876</td>\n",
       "      <td>0.010413</td>\n",
       "      <td>-0.004832</td>\n",
       "      <td>0.034466</td>\n",
       "      <td>-0.071080</td>\n",
       "      <td>0.073750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043476</td>\n",
       "      <td>-0.076574</td>\n",
       "      <td>0.039202</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>-0.015504</td>\n",
       "      <td>0.040497</td>\n",
       "      <td>-0.113724</td>\n",
       "      <td>0.067692</td>\n",
       "      <td>0.015220</td>\n",
       "      <td>-0.046457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>0.163189</td>\n",
       "      <td>0.033339</td>\n",
       "      <td>0.033599</td>\n",
       "      <td>-0.092622</td>\n",
       "      <td>-0.071083</td>\n",
       "      <td>0.030324</td>\n",
       "      <td>-0.021714</td>\n",
       "      <td>-0.060533</td>\n",
       "      <td>0.053720</td>\n",
       "      <td>0.033975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.032454</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>0.079011</td>\n",
       "      <td>-0.049759</td>\n",
       "      <td>0.009575</td>\n",
       "      <td>-0.021592</td>\n",
       "      <td>-0.008963</td>\n",
       "      <td>-0.052697</td>\n",
       "      <td>-0.016109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>804 rows × 312 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0   -0.047982 -0.007989  0.019273 -0.064445 -0.047470  0.079242 -0.094084   \n",
       "1    0.048708  0.038768  0.008993 -0.043480 -0.011525 -0.074801  0.068448   \n",
       "2    0.046934 -0.007776 -0.034438 -0.060966  0.001300 -0.036679  0.076556   \n",
       "3   -0.044973 -0.031754  0.013514 -0.088200  0.005767  0.006182  0.086622   \n",
       "4    0.019812  0.003509  0.010675 -0.081504 -0.021689 -0.016962  0.031124   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "799  0.101509 -0.086583  0.037290 -0.097883 -0.067217 -0.012579  0.081658   \n",
       "800  0.015678 -0.010740 -0.001362 -0.025951 -0.016946  0.006844 -0.015210   \n",
       "801  0.017429 -0.018824  0.004258 -0.043123  0.043978 -0.012925 -0.038624   \n",
       "802 -0.004265  0.044574  0.046853 -0.093753 -0.026876  0.010413 -0.004832   \n",
       "803  0.163189  0.033339  0.033599 -0.092622 -0.071083  0.030324 -0.021714   \n",
       "\n",
       "          7         8         9    ...       302       303       304  \\\n",
       "0    0.037180  0.049691 -0.039148  ...  0.050658 -0.002591 -0.030989   \n",
       "1   -0.029761  0.038171 -0.086254  ...  0.015159 -0.086703  0.058923   \n",
       "2   -0.046489 -0.026749 -0.019772  ...  0.004219 -0.030804 -0.031761   \n",
       "3    0.048924 -0.041500 -0.050908  ... -0.058033 -0.037874 -0.066198   \n",
       "4    0.072162 -0.058702 -0.041161  ... -0.032649 -0.088169 -0.050347   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "799  0.014439 -0.060402 -0.037441  ... -0.020561 -0.054927  0.003964   \n",
       "800  0.062379  0.035687 -0.057801  ... -0.027969 -0.004778 -0.050198   \n",
       "801  0.043852 -0.117673  0.061700  ...  0.001252 -0.078440 -0.072294   \n",
       "802  0.034466 -0.071080  0.073750  ...  0.043476 -0.076574  0.039202   \n",
       "803 -0.060533  0.053720  0.033975  ...  0.000949  0.032454 -0.001005   \n",
       "\n",
       "          305       306       307       308       309       310       311  \n",
       "0    0.011156 -0.025440  0.075424  0.062284  0.021909  0.038568 -0.067602  \n",
       "1   -0.156870  0.034332  0.029744 -0.051479  0.020975 -0.005021 -0.045287  \n",
       "2   -0.056468  0.068399  0.162177 -0.056490  0.092742  0.047726 -0.021333  \n",
       "3   -0.072266  0.083422  0.040231 -0.079380  0.028450  0.032081  0.027410  \n",
       "4   -0.059466  0.042321  0.087201 -0.081285  0.027309  0.088323 -0.029691  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "799 -0.058199  0.079025  0.088163  0.000104  0.089325  0.038812  0.045627  \n",
       "800 -0.040739  0.081978  0.084395  0.013094  0.002331  0.059930 -0.015127  \n",
       "801 -0.057007  0.068025  0.063575 -0.018018  0.005897  0.067275 -0.090942  \n",
       "802  0.005576 -0.015504  0.040497 -0.113724  0.067692  0.015220 -0.046457  \n",
       "803  0.079011 -0.049759  0.009575 -0.021592 -0.008963 -0.052697 -0.016109  \n",
       "\n",
       "[804 rows x 312 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings for test dataset:\n",
    "    \n",
    "labels_test=test_data['label_test']\n",
    "emb_list_test_ = [np.asarray(s) for s in emb_list_test]\n",
    "df_emb_test_=pd.DataFrame(emb_list_test_)\n",
    "df_emb_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc39dc6",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b49d4ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76264768, 1.45184544])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import numpy as np\n",
    "class_weight = compute_class_weight(\n",
    "    class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a89c0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=SVC(class_weight={0: 0.7626476825204483,\n",
       "                                         1: 1.4518454440599768},\n",
       "                           probability=True),\n",
       "             param_grid={&#x27;C&#x27;: [1], &#x27;gamma&#x27;: [1], &#x27;kernel&#x27;: [&#x27;poly&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=SVC(class_weight={0: 0.7626476825204483,\n",
       "                                         1: 1.4518454440599768},\n",
       "                           probability=True),\n",
       "             param_grid={&#x27;C&#x27;: [1], &#x27;gamma&#x27;: [1], &#x27;kernel&#x27;: [&#x27;poly&#x27;]},\n",
       "             scoring=&#x27;f1_macro&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight={0: 0.7626476825204483, 1: 1.4518454440599768},\n",
       "    probability=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(class_weight={0: 0.7626476825204483, 1: 1.4518454440599768},\n",
       "    probability=True)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=SVC(class_weight={0: 0.7626476825204483,\n",
       "                                         1: 1.4518454440599768},\n",
       "                           probability=True),\n",
       "             param_grid={'C': [1], 'gamma': [1], 'kernel': ['poly']},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameteres = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "parameteres = {'C': [1], 'gamma': [1],'kernel': ['poly']}\n",
    "clf = GridSearchCV(SVC(class_weight={0:class_weight[0], 1:class_weight[1]}, probability=True), param_grid=parameteres , cv=5, scoring='f1_macro')\n",
    "clf.fit(df_emb_, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40864039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from gridsearch: {'C': 1, 'gamma': 1, 'kernel': 'poly'}\n",
      "CV score=0.759\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters from gridsearch: {}\".format(clf.best_params_))\n",
    "print(\"CV score=%0.3f\" % clf.best_score_)\n",
    "cv_results = clf.cv_results_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1c42c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=clf.predict_proba(df_emb_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c81e164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def TestCutoff(df):\n",
    "\n",
    "    cut_off_list_=np.arange(0.005, 0.901, 0.005)\n",
    "#     cut_off_list = itertools.chain([0.01], cut_off_list_)\n",
    "    \n",
    "    f1score_macro_list=[]\n",
    "    f1score_list=[]\n",
    "    recall_list=[]\n",
    "    \n",
    "    predict_list_list=[[]]\n",
    "    for i, cut_off in enumerate(cut_off_list_):\n",
    "        predict_list=np.where(df['predict_1']>cut_off, 1, 0)\n",
    "        predict_list_list.append(predict_list)\n",
    "        print (cut_off)\n",
    "        precision, recall, f1score = precision_recall_fscore_support(df['label_test'], predict_list)[:3]\n",
    "        print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')\n",
    "        f1score_list.append(f1score[1])\n",
    "        recall_list.append(recall[1])\n",
    "        print (\"macro:\")\n",
    "        precision, recall, f1score_macro = precision_recall_fscore_support(df['label_test'], predict_list, average='macro')[:3]\n",
    "        print(f'precision: {precision}, recall: {recall}, f1score_macro: {f1score_macro}')\n",
    "        f1score_macro_list.append(f1score_macro)\n",
    "        print (\" \")\n",
    "    \n",
    "    max_ind=f1score_macro_list.index(max(f1score_macro_list))   \n",
    "    print (\"macro\", f1score_macro_list[max_ind])\n",
    "    print (\"F1-valued:\", f1score_list[max_ind])\n",
    "    print (\"recall-valued:\", recall_list[max_ind])\n",
    "    print (cut_off_list_[max_ind])\n",
    "    \n",
    "    df['predict']=predict_list_list[max_ind+1]\n",
    "        \n",
    "    return (df)\n",
    "#         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de725b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions0=[]\n",
    "predictions1=[]\n",
    "\n",
    "for res in y_predict:\n",
    "    predictions0.append(res[0])\n",
    "    predictions1.append(res[1])\n",
    "\n",
    "test_data['predict_0']=predictions0\n",
    "test_data['predict_1']=predictions1\n",
    "\n",
    "test_data_predict=TestCutoff(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8ae10",
   "metadata": {},
   "source": [
    "best cut-off = 0.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9042cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.875      0.78461538], recall: [0.89473684 0.75      ], f1score: [0.88475836 0.76691729]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1score = precision_recall_fscore_support(test_data.label_test, test_data['predict'])[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "383dd5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.8298076923076922, recall: 0.8223684210526316, f1score_macro: 0.8258378287726752\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1score = precision_recall_fscore_support(test_data.label_test, test_data['predict'], average='macro')[:3]\n",
    "\n",
    "print(f'precision: {precision}, recall: {recall}, f1score_macro: {f1score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c60642e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/SVC_for_binary_classification_pre-trained_rubert-tiny2_based.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "import joblib\n",
    "\n",
    "joblib.dump(clf.best_estimator_, '../models/SVC_for_binary_classification_pre-trained_rubert-tiny2_based.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
