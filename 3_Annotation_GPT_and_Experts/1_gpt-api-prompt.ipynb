{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f843233",
   "metadata": {},
   "source": [
    "Script to annotate data via gpt api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9c3a14-174b-43c4-95d8-a514b5c98e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# import api_key from .env\n",
    "import os\n",
    "print(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "import random\n",
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2575f78-6f6a-431b-b6b9-94ab9d3eb097",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7515e079-851e-4a15-b580-2b95e36b2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a professional annotator tasked with categorizing social media posts.\n",
    "Assign a binary code to each post - whether (code 1) or not (code 0) the post reflects the Values listed below:\n",
    "Self-direction: Independent thought and action—choosing, creating, and exploring.\n",
    "Stimulation: Excitement, novelty, and challenge in life.\n",
    "Hedonism: Pleasure and sensuous gratification for oneself.\n",
    "Achievement: Personal success through demonstrating competence according to social standards.\n",
    "Power: Social status and prestige, wealth, control or dominance over people and resources.\n",
    "Security: Safety, harmony, strong government, and stability of society, relationships, and self.\n",
    "Conformity: The restraint of actions, inclinations, and impulses that are likely to upset or harm others and violate social expectations or norms.\n",
    "Tradition: Respect, commitment, and acceptance of the customs and ideas that traditional culture or religion provides.\n",
    "Benevolence: Preservation and enhancement of the welfare of people with whom one is in frequent personal contact.\n",
    "Universalism: Understanding, appreciation, tolerance, and protection for the welfare of all people and of nature.\n",
    "For example, the post \"Для меня главное традиции и друзья\" expresses Value Tradition and Value Benevolence.\n",
    "For the following JSON entries of type {post_id:post_text}, return comma separated data frame with post_id as index, Value as columns and binary code in cell.\n",
    "\"\"\"\n",
    "\n",
    "instruction_end=\"\"\"\n",
    "Print comma separated data frame only without any explanation. Before printing, ensure that each post_id and each Value in the data frame has a corresponding binary code value. Data frame must have all post_id as index and all Values as columns. If errors are found, fix them.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ea6909-a24e-4301-a199-7e2151d6e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('data-to-annotate', sep=\"|\", encoding ='utf-8')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df= df.rename(columns={'level_0':\"post_id\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9ac2ea8-3281-4117-b473-097cd34afebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].str.replace('\"', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc8477d6-bf65-4e0c-9706-b5edb3a63453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate tokens number:\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_messages(messages, model):  #=\"gpt-3.5-turbo-0613\"\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        \"gpt-4-turbo\",\n",
    "        \"gpt-4-turbo-2024-04-09\",\n",
    "        \"gpt-4o-2024-05-13\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4o\" in model:\n",
    "        # print(\n",
    "        #     \"Warning: gpt-4o may update over time. Returning num tokens assuming gpt-4o-2024-05-13.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4o-2024-05-13\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c20bbd1-3426-40b5-a398-22335d31e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelectRandomExcept(df, idx_done, k):\n",
    "    \"\"\"Return list of idx, randomized. Max len = k\"\"\"\n",
    "    from_which_list_to_select=set(range(0, df.shape[0]))-set(idx_done)\n",
    "    k = min(k,len(from_which_list_to_select))\n",
    "    random_chunk=random.sample(list(from_which_list_to_select), k)\n",
    "    return [i for i in random_chunk if not i in idx_done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d8492fb-df2e-40ae-8c0f-e0540376005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a string with posts shuffled in random order\n",
    "def create_post_list_inline(instruction, instruction_end, post_dict100, model):  \n",
    "    instruction_num_token=num_tokens_from_messages(messages = [{\"role\": \"user\", \"content\": instruction}], model=model) \n",
    "    max_input_tokens=4096   #1000 - for 3.5\n",
    "    post_num_token=0\n",
    "    dict_full={}\n",
    "    i=0\n",
    "    key_list=[]\n",
    "    for key, post in post_dict100.items():\n",
    "        post_dict={}\n",
    "        post_dict[i]=post\n",
    "        post_num_token=post_num_token+num_tokens_from_messages(messages = [{\"role\": \"user\", \"content\": str(post_dict)+instruction_end}], model=model)\n",
    "        \n",
    "        if instruction_num_token+post_num_token<max_input_tokens:\n",
    "                dict_full.update(post_dict)\n",
    "                key_list.append(key)\n",
    "        if (instruction_num_token+post_num_token>max_input_tokens)&(i==0):                \n",
    "                dict_full.update(post_dict)\n",
    "                key_list.append(key)\n",
    "        if (instruction_num_token+post_num_token>max_input_tokens)&(i>0):\n",
    "            break\n",
    "        i+=1\n",
    "   \n",
    "    return dict_full, key_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "7a4918f0-e7ab-47c4-a4ce-18f590cdcb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS_OUTPUT_POST = 30\n",
    "TOKENS_OUTPUT_HEADER = 100\n",
    "\n",
    "def AreThereValuesInThePost(instruction, instruction_end, post_list_inline, model=\"gpt-4o\", n=1, post_num:int=None):\n",
    "    list_labels=[]\n",
    "    prompt=instruction+str(post_list_inline)+instruction_end\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    if post_num is None:\n",
    "        max_tokens = 1000  #1000\n",
    "    else:\n",
    "        max_tokens = TOKENS_OUTPUT_HEADER\n",
    "        max_tokens += TOKENS_OUTPUT_POST*post_num\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "                            model=model,  #\"gpt-4o\", \n",
    "                            messages=messages,\n",
    "                            max_tokens=1000,  #1000, \n",
    "                            n=n,\n",
    "                            temperature=0.1\n",
    "                        )\n",
    "    for i in range(n):\n",
    "        list_labels.append(response.choices[i].message.content)\n",
    "    # list_labels = response.choices[0].message.content.strip().split('\\n')    \n",
    "    return list_labels\n",
    "\n",
    "PLAINTEXT_PREFIX=\"```plaintext\\n\"\n",
    "PREFIX=\"```\\n\"\n",
    "POSTFIX=\"```\"\n",
    "COLUMNS_ROW = \"post_id,Self-direction,Stimulation,Hedonism,Achievement,Power,Security,Conformity,Tradition,Benevolence,Universalism\"\n",
    "columns_ref = set(COLUMNS_ROW.split(\",\")) \n",
    "# {'Achievement',\n",
    "#  'Benevolence',\n",
    "#  'Conformity',\n",
    "#  'Hedonism',\n",
    "#  'Power',\n",
    "#  'Security',\n",
    "#  'Self-direction',\n",
    "#  'Stimulation',\n",
    "#  'Tradition',\n",
    "#  'Universalism',\n",
    "#  'post_id'}\n",
    "\n",
    "def PostProcText(t: str, post_num: int, remove_plaintext: bool = True, check_columns: bool = True) -> str:\n",
    "    if remove_plaintext and t.startswith(PLAINTEXT_PREFIX) and t.endswith(POSTFIX) and (len(t) > len(PLAINTEXT_PREFIX + POSTFIX)):\n",
    "        t = t[len(PLAINTEXT_PREFIX):-len(POSTFIX)]\n",
    "    if remove_plaintext and t.startswith(PREFIX) and t.endswith(POSTFIX) and (len(t) > len(PREFIX + POSTFIX)):\n",
    "        t = t[len(PREFIX):-len(POSTFIX)]\n",
    "    if check_columns:\n",
    "        if t.startswith(COLUMNS_ROW+\"\\n\"):\n",
    "            return t\n",
    "        # else:\n",
    "        # fix columns\n",
    "        rows = t.split(\"\\n\")\n",
    "        # any col name in 1st row\n",
    "        if sum(1 for col in columns_ref if col in rows[0]):\n",
    "            # rm 1sr row\n",
    "            rows.pop(0)\n",
    "        # and fix with default one\n",
    "        rows.insert(0,COLUMNS_ROW)\n",
    "        return \"\\n\".join(rows)\n",
    "    return t\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d330858-7c95-4258-9eb2-30a638453064",
   "metadata": {},
   "source": [
    "## RUN labeling with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4605df9b-eb75-4055-85bb-09f934d33a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4\"  #\"gpt-3.5-turbo-0125\"  \"gpt-4o\"\n",
    "model_choices_n=1 # number of chat_gpt iterations\n",
    "model_try_n=5  # !!! how many times each post should be lableled with model\n",
    "chunk_rnd_size=100 #50 - for 3.5 # number of rnd numbers generated at the moment\n",
    "\n",
    "errors_num = 0\n",
    "\n",
    "model_try_df_responses = {}\n",
    "not_parsed_responses = []\n",
    "for i in range(1, model_try_n+1):\n",
    "    df_chunks = [] # list of df-s with parsed results for chunk of posts\n",
    "    idx_done=[]  # номера строк, которые уже выпадали в рандомайзере\n",
    "    while (len(idx_done)<test1.shape[0]) and (errors_num < 5):\n",
    "        chunk_idx=SelectRandomExcept(test1, idx_done, chunk_rnd_size)\n",
    "        chunk_dict=dict(zip(chunk_idx, test1.iloc[chunk_idx].text.to_list()))\n",
    "        posts_chunk_inline, chunk_idx_list=create_post_list_inline(instruction, \n",
    "                                                                   instruction_end, \n",
    "                                                                   chunk_dict, \n",
    "                                                                   model)\n",
    "\n",
    "        response_check_status = True\n",
    "        try:\n",
    "            print(chunk_idx_list)\n",
    "            print(posts_chunk_inline)\n",
    "            model_responses=AreThereValuesInThePost(instruction, \n",
    "                                                    instruction_end, \n",
    "                                                    posts_chunk_inline, \n",
    "                                                    model, \n",
    "                                                    model_choices_n,\n",
    "                                                    len(chunk_idx_list),\n",
    "                                                   )\n",
    "            print(model_responses)\n",
    "        except:\n",
    "            print('Error while promt model')\n",
    "            print(posts_chunk_inline)\n",
    "            response_check_status = False\n",
    "        try:\n",
    "            if response_check_status:\n",
    "                df_chunk_response = pd.read_csv(StringIO(PostProcText(model_responses[0], len(chunk_idx_list))), sep=',')\n",
    "        except:\n",
    "            response_check_status = False\n",
    "            not_parsed_responses.append(model_responses[0])\n",
    "        if response_check_status:\n",
    "            response_check_status = columns_ref.issubset(set(df_chunk_response.columns))\n",
    "        try:\n",
    "            df_chunk_response.index = chunk_idx_list\n",
    "            if 'post_id' in df_chunk_response.columns:\n",
    "                df_chunk_response=df_chunk_response.drop(['post_id'],axis=1)\n",
    "            elif '0' in df_chunk_response.columns:\n",
    "                df_chunk_response=df_chunk_response.drop(['post_id'],axis=1)                   \n",
    "        except:\n",
    "            response_check_status = False\n",
    "        \n",
    "        if response_check_status:\n",
    "            df_chunk_response = df_chunk_response.dropna(axis=0)\n",
    "            response_check_status = (df_chunk_response.empty == False)\n",
    "        if not response_check_status:\n",
    "            errors_num += 1\n",
    "            print('Error to parse GPT model response as CSV')            \n",
    "        else:\n",
    "            errors_num = 0\n",
    "            df_chunks.append(df_chunk_response)\n",
    "            print(df_chunk_response)\n",
    "            idx_done=idx_done+list(df_chunk_response.index)\n",
    "            \n",
    "    if len(df_chunks):        \n",
    "        model_try_df_responses[i]=pd.concat(df_chunks,axis=0).sort_index()\n",
    "# create multiIndex df for Values columns\n",
    "if len(model_try_df_responses):\n",
    "    df_responses =  pd.concat(model_try_df_responses.values(), keys=model_try_df_responses.keys(), names=['try'], axis=1)\n",
    "else:\n",
    "    df_responses = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f0fd3-5a19-4855-a764-599064a20bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat=pd.concat([test1, pd.concat([df_responses[i] for i in range (1,model_try_n+1)]).astype(str).groupby(level=0).agg(','.join)], axis=1)\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8b85c5a6-de0f-4b66-9fa3-f2c892b0ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_folder=''\n",
    "df_concat.to_csv(res_folder+'', sep='|', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
